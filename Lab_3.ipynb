{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gouN66dYSQ9e","executionInfo":{"status":"ok","timestamp":1697404518196,"user_tz":300,"elapsed":20643,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"285b8ec5-2705-4a5c-ccff-c9b6bff0a1a9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd 'drive/MyDrive/UT Austin/Courses/SLT/PS3'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-70QYI2SSEE","executionInfo":{"status":"ok","timestamp":1697404519882,"user_tz":300,"elapsed":661,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"3be68afc-4f8d-4329-d0af-19fd3eba7930"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/UT Austin/Courses/SLT/PS3\n"]}]},{"cell_type":"code","execution_count":50,"metadata":{"id":"kGztJsAbNAwU","executionInfo":{"status":"ok","timestamp":1697410893621,"user_tz":300,"elapsed":139,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}}},"outputs":[],"source":["from torch._C import AliasDb\n","import librosa\n","import math\n","import numpy as np\n","import scipy.signal\n","from scipy.special import logsumexp\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MyNet(nn.Module):\n","    def __init__(self):\n","        super(MyNet, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n","        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv4 = nn.Conv2d(64, 128, (1, 5))\n","        self.fc1 = nn.Linear(128, 128)\n","        self.fc2 = nn.Linear(128, 128)\n","        self.fc3 = nn.Linear(128, 48)\n","        self.sm = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = F.relu(self.conv4(x))\n","        x = x.view(-1, 128)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        x = self.sm(x)\n","        return x\n","\n","def load_audio_to_melspec_tensor(wavpath, sample_rate=16000):\n","    window_size = .025\n","    window_stride = 0.01\n","    n_dft = 512\n","    win_length = int(sample_rate * window_size)\n","    hop_length = int(sample_rate * window_stride)\n","    y, sr = librosa.load(wavpath, sr=sample_rate)\n","    y = y - y.mean()\n","    y = np.append(y[0],y[1:]-.97*y[:-1])\n","    # compute mel spectrogram\n","    stft = librosa.stft(y, n_fft=n_dft, hop_length=hop_length,\n","        win_length=win_length, window=scipy.signal.hamming)\n","    spec = np.abs(stft)**2\n","    mel_basis = librosa.filters.mel(sr=sample_rate, n_fft=n_dft, n_mels=40, fmin=20)\n","    melspec = np.dot(mel_basis, spec)\n","    logspec = librosa.power_to_db(melspec, ref=np.max)\n","    logspec = np.transpose(logspec)\n","    logspec_tensor = torch.tensor(logspec)\n","    return logspec_tensor\n","\n","def compute_phone_likelihoods(model, logspec):\n","    likelihood_list = []\n","    with torch.no_grad():\n","        for j in range(6, logspec.size(0) - 5):\n","            inp = logspec[j-5:j+6,:].unsqueeze(0)\n","            output = model(inp) # output will be log probabilities over classes\n","            output = output - math.log(1. / 48) # subtract the logprob of the class priors (assumed to be uniform)\n","            likelihood_list.append(output[0])\n","    likelihoods = torch.transpose(torch.stack(likelihood_list, dim=1), 0, 1).numpy()\n","    return likelihoods\n","\n","class MyHMM:\n","    def __init__(self, state_labels, initial_state_distribution, transition_matrix, eps=1e-200):\n","        self.eps = eps\n","        self.pi = np.log(initial_state_distribution + eps)\n","        self.A = np.log(transition_matrix + eps) #A_{ji} is prob of transitioning from state j to state i\n","        self.labels = state_labels # a list where self.labels[j] is the index of the phone label belonging to the jth state\n","        self.N_states = len(self.labels)\n","\n","    def forward(self, state_likelihoods):\n","        # state_likelihoods.shape is assumed to be (N_timesteps, 48)\n","        # TODO: fill in\n","        # self.N_states = i = 4\n","        # t = N_timesteps = 100\n","        # second dimention of state_likelihoods is the log likelihoods accross\n","        # the 48 different phoneme classes\n","\n","        # defining alpha, shape (100, 4), alpha[0].shape = 4\n","        alpha = np.zeros((len(state_likelihoods), self.N_states))\n","        # initialize alpha[0][i]=alpha_0(i) for i = 4 states\n","        for i in range(self.N_states):\n","          alpha[0][i] = state_likelihoods[0][self.labels[i]] + self.pi[i]\n","\n","        for t in range(1, len(state_likelihoods)): # 1,...,99\n","          for i in range(self.N_states):\n","            # TODO: confirm if it is over 48 or 4, it should be self.N_states\n","            summ = np.zeros(self.N_states)\n","            for j in range(self.N_states):\n","              summ[j] += alpha[t-1][j] + self.A[j][i]\n","            summation = logsumexp(summ)\n","            alpha[t][i] = summation + state_likelihoods[t][self.labels[i]]\n","        return alpha[len(state_likelihoods) - 1][self.N_states - 1]\n","\n","    def viterbi(self, state_likelihoods):\n","        # state_likelihoods.shape is assumed to be (N_timesteps, 48)\n","        # TODO: fill in\n","        delta = np.zeros((len(state_likelihoods), self.N_states)) # shape: (100, 4) = (T, N)\n","        # defining matrix\n","        psi = np.zeros((len(state_likelihoods), self.N_states)) # shape: (100, 4) = (T, N)\n","        # initializing\n","        for i in range(self.N_states):\n","          delta[0][i] = state_likelihoods[0][self.labels[i]] + self.pi[i]\n","        # induction step\n","        for t in range(1, len(state_likelihoods)):\n","          for i in range(self.N_states):\n","            summ = np.zeros(self.N_states)\n","            max = 0\n","            for j in range(self.N_states):\n","              summ[j] += delta[t - 1][j] + self.A[j][i]\n","            max = np.amax(summ)\n","            delta[t][i] = max + state_likelihoods[t][self.labels[i]]\n","            psi[t][i] = np.argmax(summ)\n","        # score best path\n","        p = np.amax(delta[len(state_likelihoods) - 1])\n","        # best final state\n","        q = np.zeros(len(state_likelihoods)).astype(np.int32)\n","        q[len(state_likelihoods) - 1] = np.argmax(delta[len(state_likelihoods) - 1])\n","        # backtracking\n","        for t in range(len(state_likelihoods) - 2, -1, -1):\n","          q[t] = psi[t + 1][q[t + 1]]\n","        return q\n","\n","    def viterbi_transition_update(self, state_likelihoods):\n","        # state_likelihoods.shape is assumed to be (N_timesteps, 48)\n","        # TODO: fill\n","        # hidden states should be 4 or 6\n","\n","        p = self.viterbi(state_likelihoods)\n","        eps = 1e-200\n","        self.pi = np.zeros(self.N_states)\n","        # self.pi = self.pi + eps\n","\n","        for i in range(self.N_states):\n","          if p[0] == i:\n","            self.pi[i] += 1\n","          else:\n","            self.pi[i] += eps\n","          self.pi[i] = np.log(self.pi[i])\n","\n","        for i in range(self.N_states):\n","          for j in range(self.N_states):\n","            numerator = 0\n","            denominator = 0\n","            for t in range(len(state_likelihoods) - 1):\n","              if t < len(state_likelihoods) - 1 and p[t] == i and p[t + 1] == j:\n","                numerator += 1\n","              if p[t] == i:\n","                denominator += 1\n","            self.A[i][j] = np.log(numerator/denominator + eps)\n","\n","        return\n","\n","model = MyNet()\n","model.load_state_dict(torch.load('lab3_AM.pt'))\n","\n","lab3_data = np.load('lab3_phone_labels.npz')\n","\n","phone_labels = list(lab3_data['phone_labels'])\n","\n","def phones2indices(phones):\n","    return [phone_labels.index(p) for p in phones]\n","\n","fee_HMM = MyHMM(phones2indices(['sil', 'f', 'iy', 'sil']), np.array([0.5, 0.5, 0, 0]), np.array([[.9,.1,0,0],[0,.9,.1,0],[0,0,.9,.1],[0,0,0,1]]))\n","pea_HMM = MyHMM(phones2indices(['sil', 'p', 'iy', 'sil']), np.array([0.5, 0.5, 0, 0]), np.array([[.9,.1,0,0],[0,.9,.1,0],[0,0,.9,.1],[0,0,0,1]]))\n","rock_HMM = MyHMM(phones2indices(['sil', 'r', 'aa', 'cl', 'k', 'sil']), np.array([0.5,0.5,0,0,0,0]), np.array([[.9,.1,0,0,0,0],[0,.9,.1,0,0,0],[0,0,.9,.1,0,0],[0,0,0,.9,.1,0],[0,0,0,0,.9,.1],[0,0,0,0,0,1]]))\n","burt_HMM = MyHMM(phones2indices(['sil', 'b', 'er', 'cl', 't', 'sil']), np.array([0.5,0.5,0,0,0,0]), np.array([[.9,.1,0,0,0,0],[0,.9,.1,0,0,0],[0,0,.9,.1,0,0],[0,0,0,.9,.1,0],[0,0,0,0,.9,.1],[0,0,0,0,0,1]]))\n","see_HMM = MyHMM(phones2indices(['sil', 's', 'iy', 'sil']), np.array([0.5, 0.5, 0, 0]), np.array([[.9,.1,0,0],[0,.9,.1,0],[0,0,.9,.1],[0,0,0,1]]))\n","she_HMM = MyHMM(phones2indices(['sil', 'sh', 'iy', 'sil']), np.array([0.5, 0.5, 0, 0]), np.array([[.9,.1,0,0],[0,.9,.1,0],[0,0,.9,.1],[0,0,0,1]]))\n","\n","# TODO: write your code to use your HMMs below here (or in a new block)"]},{"cell_type":"code","source":["# computing the likelihoods\n","burt_likelihoods = compute_phone_likelihoods(model, load_audio_to_melspec_tensor(\"burt.wav\"))\n","fee_likelihoods = compute_phone_likelihoods(model, load_audio_to_melspec_tensor(\"fee.wav\"))\n","pea_likelihoods = compute_phone_likelihoods(model, load_audio_to_melspec_tensor(\"pea.wav\"))\n","rock_likelihoods = compute_phone_likelihoods(model, load_audio_to_melspec_tensor(\"rock.wav\"))\n","see_likelihoods = compute_phone_likelihoods(model, load_audio_to_melspec_tensor(\"see.wav\"))\n","she_likelihoods = compute_phone_likelihoods(model, load_audio_to_melspec_tensor(\"she.wav\"))\n","\n","hmms = [fee_HMM, pea_HMM, rock_HMM, burt_HMM, see_HMM, she_HMM]\n","wavs = [fee_likelihoods, pea_likelihoods, rock_likelihoods, burt_likelihoods, see_likelihoods, she_likelihoods]\n","\n","likelihood_table = np.zeros((6,6))\n","\n","for i in range(len(wavs)):\n","  for j in range(len(hmms)):\n","    likelihood_table[i, j] = (hmms[j].forward(wavs[i]))\n","likelihood_table = np.around(likelihood_table, 2)\n","print(likelihood_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r03jG-iyZNUe","executionInfo":{"status":"ok","timestamp":1697410915719,"user_tz":300,"elapsed":2326,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"a0adf748-6178-4d76-a3e4-99e06fd257d0"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 208.75  179.36  -92.06  -85.49  192.24  182.34]\n"," [ 250.02  268.16    7.83   71.11  236.17  234.84]\n"," [ -62.59   -5.84  158.39   66.67  -61.83  -63.55]\n"," [ -62.84  -19.97  116.55  221.06  -75.79 -100.31]\n"," [  72.8    75.08 -177.84 -160.09  229.06  123.82]\n"," [  78.29   91.94 -211.49 -191.97  124.63  281.11]]\n"]}]},{"cell_type":"code","source":["optimal_hidden_state_rock = rock_HMM.viterbi(rock_likelihoods)\n","print(optimal_hidden_state_rock)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvRKomMGdfTd","executionInfo":{"status":"ok","timestamp":1697410917137,"user_tz":300,"elapsed":160,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"56002d7d-2539-47ee-9f76-0f4dca03663f"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n"," 5 5 5 5 5 5 5]\n"]}]},{"cell_type":"code","source":["first_transition_matrix = np.around(np.exp(rock_HMM.A), 2)\n","print(first_transition_matrix)\n","first_likelihoods_rock = rock_HMM.forward(rock_likelihoods)\n","print(\"likelihood 1: \", first_likelihoods_rock)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAxDNRuVg0u2","executionInfo":{"status":"ok","timestamp":1697410919068,"user_tz":300,"elapsed":153,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"e038cda3-c13e-42a8-9b16-98060e33253a"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.9 0.1 0.  0.  0.  0. ]\n"," [0.  0.9 0.1 0.  0.  0. ]\n"," [0.  0.  0.9 0.1 0.  0. ]\n"," [0.  0.  0.  0.9 0.1 0. ]\n"," [0.  0.  0.  0.  0.9 0.1]\n"," [0.  0.  0.  0.  0.  1. ]]\n","likelihood:  158.3910544157233\n"]}]},{"cell_type":"code","source":["# Viterbi update\n","rock_HMM.viterbi_transition_update(rock_likelihoods)\n","second_transition_matrix = np.around(np.exp(rock_HMM.A), 2)\n","print(second_transition_matrix)\n","# computing likelihood\n","second_likelihoods_rock = rock_HMM.forward(rock_likelihoods)\n","print(\"likelihood 2: \", second_likelihoods_rock)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZ-2C85QfZpG","executionInfo":{"status":"ok","timestamp":1697410948364,"user_tz":300,"elapsed":132,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"538e456b-9548-4b1e-e6cb-7825b8b1d82a"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.93 0.07 0.   0.   0.   0.  ]\n"," [0.   0.92 0.08 0.   0.   0.  ]\n"," [0.   0.   0.92 0.08 0.   0.  ]\n"," [0.   0.   0.   0.92 0.08 0.  ]\n"," [0.   0.   0.   0.   0.8  0.2 ]\n"," [0.   0.   0.   0.   0.   1.  ]]\n","likelihood 2:  159.5448084880059\n"]}]},{"cell_type":"code","source":["print(\"First transition matrix: \\n\", first_transition_matrix)\n","print(\"Second transition matrix: \\n\", second_transition_matrix)\n","print(\"Likelihood 1: \", first_likelihoods_rock)\n","print(\"Likelihood 2: \", second_likelihoods_rock)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed0KwjtLk6SY","executionInfo":{"status":"ok","timestamp":1697413782438,"user_tz":300,"elapsed":139,"user":{"displayName":"sebastian gaete","userId":"13034370892726844972"}},"outputId":"9f105be3-25e1-4d55-dbb8-d60752b29467"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["First transition matrix: \n"," [[0.9 0.1 0.  0.  0.  0. ]\n"," [0.  0.9 0.1 0.  0.  0. ]\n"," [0.  0.  0.9 0.1 0.  0. ]\n"," [0.  0.  0.  0.9 0.1 0. ]\n"," [0.  0.  0.  0.  0.9 0.1]\n"," [0.  0.  0.  0.  0.  1. ]]\n","Second transition matrix: \n"," [[0.93 0.07 0.   0.   0.   0.  ]\n"," [0.   0.92 0.08 0.   0.   0.  ]\n"," [0.   0.   0.92 0.08 0.   0.  ]\n"," [0.   0.   0.   0.92 0.08 0.  ]\n"," [0.   0.   0.   0.   0.8  0.2 ]\n"," [0.   0.   0.   0.   0.   1.  ]]\n","Likelihood 1:  158.3910544157233\n","Likelihood 2:  159.5448084880059\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}